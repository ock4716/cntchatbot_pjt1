"""
qa_system.py
[6ë‹¨ê³„ í†µí•©] LLM í†µí•© - ëŒ€í™” íˆìŠ¤í† ë¦¬ ì§€ì› ë²„ì „

ê²€ìƒ‰ ê²°ê³¼ë¥¼ LLMì— ì „ë‹¬í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ìƒì„±
- ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ…
- ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
- í”„ë¡¬í”„íŠ¸ ê´€ë¦¬
- LLM í˜¸ì¶œ (í…ìŠ¤íŠ¸ ë‹µë³€ë§Œ)
- ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬ (ì—°ì†ì ì¸ ì§ˆì˜ì‘ë‹µ)
"""

from openai import OpenAI
from typing import List, Dict, Optional


class QASystem:
    """Q&A ì‹œìŠ¤í…œ í†µí•© í´ë˜ìŠ¤ (ëŒ€í™” íˆìŠ¤í† ë¦¬ ì§€ì›)"""
    
    def __init__(self, openai_api_key: str, model: str = "gpt-4o"):
        """
        QASystem ì´ˆê¸°í™”
        
        Args:
            openai_api_key: OpenAI API í‚¤
            model: ì‚¬ìš©í•  ëª¨ë¸ëª…
        """
        self.client = OpenAI(api_key=openai_api_key)
        self.model = model
        self.system_prompt = self._create_system_prompt()
        self.conversation_history = []  # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥
        print(f"âœ“ QASystem ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {model})")
    
    def _create_system_prompt(self) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ëŒ€í™” ì „ìš©)"""
        return """ë‹¹ì‹ ì€ ë¶€ë™ì‚° ì‹œì¥ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ë¶€ë™ì‚° ë¦¬í¬íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ë¬´ìë“¤ì—ê²Œ ëª…í™•í•˜ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

    ë‹µë³€ ìŠ¤íƒ€ì¼:
    1. ì¹œê·¼í•˜ì§€ë§Œ ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”
    2. ë¶ˆí•„ìš”í•œ ê²©ì‹ì€ ìƒëµí•˜ê³  í•µì‹¬ë§Œ ì „ë‹¬í•˜ì„¸ìš”
    3. ì§ˆë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•´ì„œ ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”
    4. ì´ì „ ëŒ€í™”ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ì„¸ìš”

    ë‹µë³€ ê·œì¹™:
    1. ë¦¬í¬íŠ¸ì— ìˆëŠ” ë‚´ìš©ë§Œ ë‹µë³€í•˜ì„¸ìš”
    2. ìˆ˜ì¹˜ëŠ” ì •í™•í•˜ê²Œ ì¸ìš©í•˜ì„¸ìš”
    3. ì¤‘ìš”í•œ ì •ë³´ ë’¤ì—ëŠ” [1], [2] í˜•íƒœë¡œ ì¶œì²˜ë¥¼ í‘œê¸°í•˜ì„¸ìš”
    4. ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ì†”ì§í•˜ê²Œ "ë¦¬í¬íŠ¸ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  í•˜ì„¸ìš”
    5. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”

    ì¶œì²˜ í‘œê¸°:
    - ë‹µë³€ ëì— ê°„ë‹¨íˆ ì¶œì²˜ ëª©ë¡ ì‘ì„±

    ë‹µë³€ ì˜ˆì‹œ:
    2024ë…„ ì„œìš¸ ì•„íŒŒíŠ¸ ë§¤ë§¤ê°€ê²©ì€ 2.0% ì˜¬ëìŠµë‹ˆë‹¤. [1] íŠ¹íˆ ê°•ë‚¨êµ¬ëŠ” ì „ê³ ì ì„ ë„˜ì–´ì„°ë„¤ìš”. [2]

    ì¶œì²˜:
    [1] KB ë¦¬í¬íŠ¸, í‘œâ… -2. ì§€ì—­ë³„ ì£¼íƒ ë§¤ë§¤ê°€ê²© ë³€ë™ë¥  (12í˜ì´ì§€)
    [2] KB ë¦¬í¬íŠ¸, ë³¸ë¬¸ (25í˜ì´ì§€)

    ì¶”ì²œ ì§ˆë¬¸ì— ë‹µí•  ë•Œ:
    - "ì§ˆë¬¸ ì¶”ì²œí•´ë“œë¦´ê¹Œìš”?" ê°™ì€ ê°„ë‹¨í•œ ì œì•ˆ
    - 2-3ê°œ í•µì‹¬ ì§ˆë¬¸ë§Œ ì¶”ì²œ
    - í˜ì´ì§€ ë²ˆí˜¸ëŠ” í•„ìš”í•  ë•Œë§Œ ì–¸ê¸‰
    """

    def add_to_history(self, role: str, content: str):
        """
        ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ë©”ì‹œì§€ ì¶”ê°€
        
        Args:
            role: 'user' ë˜ëŠ” 'assistant'
            content: ë©”ì‹œì§€ ë‚´ìš©
        """
        self.conversation_history.append({
            "role": role,
            "content": content
        })
    
    def get_conversation_history(self) -> List[Dict]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return self.conversation_history
    
    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_history = []
        print("âœ“ ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def rewrite_query(self, query: str) -> str:
        """
        ì¿¼ë¦¬ë¥¼ ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ë¦¬ë¼ì´íŒ…
        
        Args:
            query: ì›ë³¸ ì¿¼ë¦¬
        
        Returns:
            ìµœì í™”ëœ ì¿¼ë¦¬
        """
        prompt = f"""ë‹¹ì‹ ì€ ë¶€ë™ì‚° ë¦¬í¬íŠ¸ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ì ì§ˆë¬¸ì„ ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
- êµ¬ì–´ì²´ë¥¼ ë¬¸ì–´ì²´ë¡œ ë³€í™˜
- í‚¤ì›Œë“œë¥¼ ëª…í™•í•˜ê²Œ
- ê´€ë ¨ ë™ì˜ì–´ ì¶”ê°€
- ê°„ê²°í•˜ê²Œ (1-2ë¬¸ì¥)

ì›ë˜ ì§ˆë¬¸: {query}

ìµœì í™”ëœ ì§ˆë¬¸:"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=100
            )
            
            rewritten = response.choices[0].message.content.strip()
            print(f"\nğŸ”„ ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ…:")
            print(f"  ì›ë³¸: {query}")
            print(f"  ë³€í™˜: {rewritten}")
            
            return rewritten
            
        except Exception as e:
            print(f"âš  ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ… ì‹¤íŒ¨: {e}")
            return query
    
    def build_context(self, search_results: List[Dict], max_chunks: int = 5) -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        
        Args:
            search_results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            max_chunks: ìµœëŒ€ ì²­í¬ ìˆ˜
        
        Returns:
            êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
        """
        if not search_results:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        top_results = search_results[:max_chunks]
        
        context_parts = ["ë‹¤ìŒì€ 2024 KB ë¶€ë™ì‚° ë¦¬í¬íŠ¸ì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:\n"]
        
        for i, result in enumerate(top_results, 1):
            metadata = result.get("metadata", {})
            content = result.get("content", "")
            
            # ê¸°ê´€ ì •ë³´
            institution = metadata.get("institution", "unknown")
            institution_map = {
                "hd": "HD í˜„ëŒ€ ë¦¬í¬íŠ¸",
                "kb": "KB ë¶€ë™ì‚° ë¦¬í¬íŠ¸",
                "khi": "KHI ì£¼íƒê¸ˆìœµ ë¦¬í¬íŠ¸"
            }
            source_name = institution_map.get(institution, f"{institution} ë¦¬í¬íŠ¸")
            
            # ë¬¸ì„œ íƒ€ì…
            doc_type_map = {
                "text": "ë³¸ë¬¸",
                "table": "í‘œ",
                "image": "ê·¸ë˜í”„/ì´ë¯¸ì§€"
            }
            doc_type = doc_type_map.get(metadata.get("doc_type"), "ë³¸ë¬¸")
            page = metadata.get("page", "unknown")
            
            # ì¶”ê°€ ì •ë³´ (ìˆëŠ” ê²½ìš°)
            extra_info = ""
            if metadata.get("table_id"):
                extra_info = f"\ní‘œ ID: {metadata.get('table_id')}"
            elif metadata.get("image_path"):
                image_path = metadata.get('image_path')
                image_filename = image_path.split('\\')[-1] if '\\' in image_path else image_path.split('/')[-1]
                extra_info = f"\nì´ë¯¸ì§€: {image_filename}"
            
            formatted = f"""[ì»¨í…ìŠ¤íŠ¸ {i}]
ì¶œì²˜ ê¸°ê´€: {source_name}
íƒ€ì…: {doc_type}
í˜ì´ì§€: {page}í˜ì´ì§€{extra_info}

ë‚´ìš©:
{content}

ì¶œì²˜: [{i}] {source_name} {doc_type} ({page}í˜ì´ì§€)
"""
            context_parts.append(formatted)
            context_parts.append("â”€" * 80 + "\n")
        
        full_context = "\n".join(context_parts)
        
        print(f"\nğŸ“„ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ:")
        print(f"  - ì´ ì²­í¬ ìˆ˜: {len(top_results)}")
        print(f"  - í…ìŠ¤íŠ¸: {len([r for r in top_results if r.get('metadata', {}).get('doc_type') == 'text'])}")
        print(f"  - í‘œ: {len([r for r in top_results if r.get('metadata', {}).get('doc_type') == 'table'])}")
        print(f"  - ì´ë¯¸ì§€: {len([r for r in top_results if r.get('metadata', {}).get('doc_type') == 'image'])}")
        
        return full_context
    
    def generate_answer(self, query: str, context: str, 
                       temperature: float = 0.3,
                       max_tokens: int = 2000,
                       use_history: bool = True) -> Optional[str]:
        """
        LLMìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„± (ëŒ€í™” íˆìŠ¤í† ë¦¬ ì§€ì›)
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context: êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸
            temperature: ì˜¨ë„ (0.0-2.0)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            use_history: ëŒ€í™” íˆìŠ¤í† ë¦¬ ì‚¬ìš© ì—¬ë¶€
        
        Returns:
            í…ìŠ¤íŠ¸ ë‹µë³€
        """
        user_prompt = f"""{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ìœ„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ì¶œì²˜ ë²ˆí˜¸ [1], [2] ë“±ì„ ëª…ì‹œí•˜ì„¸ìš”."""

        try:
            print(f"\nğŸ¤– LLM í˜¸ì¶œ ì¤‘... (ëª¨ë¸: {self.model}, íˆìŠ¤í† ë¦¬: {use_history})")
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [{"role": "system", "content": self.system_prompt}]
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (use_historyê°€ Trueì¼ ë•Œë§Œ)
            if use_history and self.conversation_history:
                # ìµœê·¼ 4ê°œì˜ ëŒ€í™”ë§Œ í¬í•¨ (ë„ˆë¬´ ê¸¸ì–´ì§€ëŠ” ê²ƒ ë°©ì§€)
                recent_history = self.conversation_history[-8:]  # user + assistant ìŒ 4ê°œ
                messages.extend(recent_history)
                print(f"  - ëŒ€í™” íˆìŠ¤í† ë¦¬ {len(recent_history)}ê°œ ë©”ì‹œì§€ í¬í•¨")
            
            # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
            messages.append({"role": "user", "content": user_prompt})
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            answer = response.choices[0].message.content
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            if use_history:
                self.add_to_history("user", query)
                self.add_to_history("assistant", answer)
            
            usage = response.usage
            print(f"âœ“ LLM ì‘ë‹µ ì™„ë£Œ")
            print(f"  - ì…ë ¥ í† í°: {usage.prompt_tokens}")
            print(f"  - ì¶œë ¥ í† í°: {usage.completion_tokens}")
            print(f"  - ì´ í† í°: {usage.total_tokens}")
            print(f"  - í˜„ì¬ ëŒ€í™” í„´ ìˆ˜: {len(self.conversation_history) // 2}")
            
            return answer
            
        except Exception as e:
            print(f"âœ— LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def answer_question(self, query: str, search_results: List[Dict],
                       rewrite: bool = True,
                       use_history: bool = True,
                       temperature: float = 0.3) -> str:
        """
        ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ (ëŒ€í™” íˆìŠ¤í† ë¦¬ ì§€ì›)
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            search_results: ê²€ìƒ‰ ê²°ê³¼
            rewrite: ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ… ì‚¬ìš© ì—¬ë¶€
            use_history: ëŒ€í™” íˆìŠ¤í† ë¦¬ ì‚¬ìš© ì—¬ë¶€
            temperature: ìƒì„± ì˜¨ë„
        
        Returns:
            í…ìŠ¤íŠ¸ ë‹µë³€
        """
        print("\n" + "="*80)
        print(f"â“ ì§ˆë¬¸: {query}")
        print("="*80)
        
        # 1. ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ… (ì„ íƒ)
        search_query = query
        if rewrite:
            search_query = self.rewrite_query(query)
        
        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self.build_context(search_results)
        
        # 3. LLM ë‹µë³€ ìƒì„± (ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨)
        answer = self.generate_answer(
            search_query, 
            context, 
            use_history=use_history,
            temperature=temperature
        )
        
        if not answer:
            return "ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
        
        # 4. ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*80)
        print("ğŸ’¡ ë‹µë³€:")
        print("="*80)
        print(answer)
        print("="*80)
        
        return answer