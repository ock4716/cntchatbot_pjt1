"""
embedding_manager.py
[5ë‹¨ê³„] ì„ë² ë”© ë° FAISS

ì„ë² ë”© ìƒì„±ê³¼ FAISS ì¸ë±ìŠ¤ ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.
- OpenAI ì„ë² ë”© ìƒì„± (text-embedding-3-large)
- FAISS ì¸ë±ìŠ¤ ìƒì„±/ë¡œë“œ
- ë©”íƒ€ë°ì´í„° ê´€ë¦¬
- ì„ë² ë”© ìºì‹±
"""

import os
import json
import pickle
import hashlib
from typing import List, Dict, Optional, Tuple
import numpy as np
from openai import OpenAI
import faiss


class EmbeddingManager:
    """ì„ë² ë”© ìƒì„± ë° FAISS ì¸ë±ìŠ¤ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 openai_api_key: str,
                 institution: str = "unknown",  # â† ì¶”ê°€
                 model: str = "text-embedding-3-large",
                 cache_path: str = None,  # â† Noneìœ¼ë¡œ ë³€ê²½
                 dimension: int = 3072):
        """
        EmbeddingManager ì´ˆê¸°í™”
        
        Args:
            openai_api_key: OpenAI API í‚¤
            model: ì„ë² ë”© ëª¨ë¸ëª…
            cache_path: ì„ë² ë”© ìºì‹œ íŒŒì¼ ê²½ë¡œ
            dimension: ì„ë² ë”© ì°¨ì› (text-embedding-3-large = 3072)
        """
        self.client = OpenAI(api_key=openai_api_key)
        self.model = model
        self.institution = institution  # â† ì¶”ê°€
    
        # ìºì‹œ ê²½ë¡œ ìë™ ìƒì„± (ê¸°ê´€ë³„)
        if cache_path is None:
            cache_path = f"data/cache/embeddings_{institution}.pkl"

        self.cache_path = cache_path
        self.dimension = dimension
        self.embedding_cache = self.load_embedding_cache()
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(cache_path), exist_ok=True)
        
        print(f"âœ“ EmbeddingManager ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"  - ëª¨ë¸: {model}")
        print(f"  - ì°¨ì›: {dimension}")
        print(f"  - ìºì‹œ: {len(self.embedding_cache)}ê°œ ì„ë² ë”©")
    
    def load_embedding_cache(self) -> Dict[str, np.ndarray]:
        """
        ì„ë² ë”© ìºì‹œ ë¡œë“œ
        
        Returns:
            ìºì‹œ ë”•ì…”ë„ˆë¦¬ {text_hash: embedding_vector}
        """
        if os.path.exists(self.cache_path):
            try:
                with open(self.cache_path, 'rb') as f:
                    cache = pickle.load(f)
                print(f"âœ“ ìºì‹œ ë¡œë“œ: {len(cache)}ê°œ ì„ë² ë”©")
                return cache
            except Exception as e:
                print(f"âš  ìºì‹œ ë¡œë“œ ì‹¤íŒ¨ ({e}), ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
                return {}
        return {}
    
    def save_embedding_cache(self):
        """ì„ë² ë”© ìºì‹œ ì €ì¥"""
        try:
            with open(self.cache_path, 'wb') as f:
                pickle.dump(self.embedding_cache, f)
            print(f"âœ“ ìºì‹œ ì €ì¥: {len(self.embedding_cache)}ê°œ ì„ë² ë”©")
        except Exception as e:
            print(f"âš  ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_text_hash(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ì˜ MD5 í•´ì‹œ ê³„ì‚° (ìºì‹œ í‚¤ë¡œ ì‚¬ìš©)
        
        Args:
            text: í•´ì‹œë¥¼ ê³„ì‚°í•  í…ìŠ¤íŠ¸
        
        Returns:
            MD5 í•´ì‹œ ë¬¸ìì—´
        """
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def embed_text(self, text: str) -> np.ndarray:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        
        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸
        
        Returns:
            ì„ë² ë”© ë²¡í„° (numpy array)
        """
        # ìºì‹œ í™•ì¸
        text_hash = self.get_text_hash(text)
        if text_hash in self.embedding_cache:
            return self.embedding_cache[text_hash]
        
        # OpenAI API í˜¸ì¶œ
        try:
            response = self.client.embeddings.create(
                input=text,
                model=self.model
            )
            
            embedding = np.array(response.data[0].embedding, dtype='float32')
            
            # ìºì‹œì— ì €ì¥
            self.embedding_cache[text_hash] = embedding
            
            return embedding
            
        except Exception as e:
            print(f"âš  ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ì œë¡œ ë²¡í„° ë°˜í™˜
            return np.zeros(self.dimension, dtype='float32')
    
    def embed_chunks(self, chunks: List[Dict], batch_size: int = 100) -> Tuple[List[np.ndarray], List[str]]:
        """
        ì—¬ëŸ¬ ì²­í¬ë¥¼ ë°°ì¹˜ë¡œ ì„ë² ë”©
        
        Args:
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸° (OpenAI APIëŠ” ìµœëŒ€ 2048ê°œê¹Œì§€ ì§€ì›)
        
        Returns:
            (ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸, ì²­í¬ ID ë¦¬ìŠ¤íŠ¸)
        """
        embeddings = []
        chunk_ids = []
        
        print(f"\nğŸ“Š ì„ë² ë”© ìƒì„± ì‹œì‘...")
        print(f"  - ì´ ì²­í¬ ìˆ˜: {len(chunks)}")
        print(f"  - ë°°ì¹˜ í¬ê¸°: {batch_size}")
        
        # ìºì‹œ íˆíŠ¸/ë¯¸ìŠ¤ ì¹´ìš´íŠ¸
        cache_hits = 0
        cache_misses = 0
        
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i+batch_size]
            batch_texts = [chunk['content'] for chunk in batch]
            batch_chunk_ids = [chunk['chunk_id'] for chunk in batch]
            
            print(f"\n  ë°°ì¹˜ {i//batch_size + 1}/{(len(chunks)-1)//batch_size + 1} ì²˜ë¦¬ ì¤‘...")
            
            # ë°°ì¹˜ ë‚´ì—ì„œ ìºì‹œ í™•ì¸
            batch_embeddings = []
            texts_to_embed = []
            text_indices = []
            
            for j, text in enumerate(batch_texts):
                text_hash = self.get_text_hash(text)
                if text_hash in self.embedding_cache:
                    batch_embeddings.append(self.embedding_cache[text_hash])
                    cache_hits += 1
                else:
                    batch_embeddings.append(None)  # ë‚˜ì¤‘ì— ì±„ìš¸ ìë¦¬
                    texts_to_embed.append(text)
                    text_indices.append(j)
                    cache_misses += 1
            
            # ìºì‹œì— ì—†ëŠ” ê²ƒë§Œ API í˜¸ì¶œ
            if texts_to_embed:
                try:
                    response = self.client.embeddings.create(
                        input=texts_to_embed,
                        model=self.model
                    )
                    
                    # ê²°ê³¼ë¥¼ í•´ë‹¹ ìœ„ì¹˜ì— ì±„ìš°ê¸°
                    for idx, data in enumerate(response.data):
                        embedding = np.array(data.embedding, dtype='float32')
                        original_idx = text_indices[idx]
                        batch_embeddings[original_idx] = embedding
                        
                        # ìºì‹œì— ì €ì¥
                        text_hash = self.get_text_hash(texts_to_embed[idx])
                        self.embedding_cache[text_hash] = embedding
                    
                    print(f"    âœ“ {len(texts_to_embed)}ê°œ ìƒˆë¡œ ì„ë² ë”© ìƒì„±")
                    
                except Exception as e:
                    print(f"    âœ— ë°°ì¹˜ ì„ë² ë”© ì‹¤íŒ¨: {e}")
                    # ì‹¤íŒ¨í•œ ê²½ìš° ì œë¡œ ë²¡í„°ë¡œ ì±„ìš°ê¸°
                    for idx in text_indices:
                        if batch_embeddings[idx] is None:
                            batch_embeddings[idx] = np.zeros(self.dimension, dtype='float32')
            
            embeddings.extend(batch_embeddings)
            chunk_ids.extend(batch_chunk_ids)
            
            # ì§„í–‰ë¥  ì¶œë ¥
            progress = min((i + batch_size) / len(chunks) * 100, 100)
            print(f"    ì§„í–‰ë¥ : {progress:.1f}%")
        
        print(f"\nâœ“ ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
        print(f"  - ìºì‹œ íˆíŠ¸: {cache_hits}ê°œ")
        print(f"  - ìƒˆë¡œ ìƒì„±: {cache_misses}ê°œ")
        print(f"  - ì´ ì„ë² ë”©: {len(embeddings)}ê°œ")
        
        # ìºì‹œ ì €ì¥
        if cache_misses > 0:
            self.save_embedding_cache()
        
        return embeddings, chunk_ids
    
    def create_faiss_index(self, embeddings: List[np.ndarray]) -> faiss.Index:
        """
        FAISS ì¸ë±ìŠ¤ ìƒì„±
        
        Args:
            embeddings: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        
        Returns:
            FAISS ì¸ë±ìŠ¤
        """
        print(f"\nğŸ”§ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        # Flat ì¸ë±ìŠ¤ ìƒì„± (ì •í™•í•œ ìµœê·¼ì ‘ ì´ì›ƒ ê²€ìƒ‰)
        index = faiss.IndexFlatL2(self.dimension)
        
        # ì„ë² ë”©ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
        embeddings_array = np.array(embeddings).astype('float32')
        
        # ì¸ë±ìŠ¤ì— ì¶”ê°€
        index.add(embeddings_array)
        
        print(f"âœ“ FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        print(f"  - ì¸ë±ìŠ¤ íƒ€ì…: Flat (L2 distance)")
        print(f"  - ë²¡í„° ìˆ˜: {index.ntotal}")
        print(f"  - ì°¨ì›: {self.dimension}")
        
        return index
    
    def save_index(self, index: faiss.Index, index_path: str):
        """
        FAISS ì¸ë±ìŠ¤ ì €ì¥
        
        Args:
            index: ì €ì¥í•  FAISS ì¸ë±ìŠ¤
            index_path: ì €ì¥ ê²½ë¡œ
        """
        os.makedirs(os.path.dirname(index_path), exist_ok=True)
        
        try:
            faiss.write_index(index, index_path)
            print(f"âœ“ ì¸ë±ìŠ¤ ì €ì¥: {index_path}")
        except Exception as e:
            print(f"âœ— ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def load_index(self, index_path: str) -> Optional[faiss.Index]:
        """
        FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        
        Args:
            index_path: ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ
        
        Returns:
            FAISS ì¸ë±ìŠ¤ ë˜ëŠ” None
        """
        if not os.path.exists(index_path):
            print(f"âš  ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {index_path}")
            return None
        
        try:
            index = faiss.read_index(index_path)
            print(f"âœ“ ì¸ë±ìŠ¤ ë¡œë“œ: {index_path}")
            print(f"  - ë²¡í„° ìˆ˜: {index.ntotal}")
            return index
        except Exception as e:
            print(f"âœ— ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def save_metadata(self, chunks: List[Dict], chunk_ids: List[str], metadata_path: str):
        """
        ë©”íƒ€ë°ì´í„° ì €ì¥
        
        Args:
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
            chunk_ids: ì²­í¬ ID ë¦¬ìŠ¤íŠ¸ (ì¸ë±ìŠ¤ ìˆœì„œì™€ ë™ì¼)
            metadata_path: ì €ì¥ ê²½ë¡œ
        """
        os.makedirs(os.path.dirname(metadata_path), exist_ok=True)
        
        # chunk_idë¥¼ í‚¤ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
        chunk_dict = {chunk['chunk_id']: chunk for chunk in chunks}
        
        # ì¸ë±ìŠ¤ ìˆœì„œëŒ€ë¡œ ë©”íƒ€ë°ì´í„° ë°°ì—´ ìƒì„±
        metadata = []
        for i, chunk_id in enumerate(chunk_ids):
            chunk = chunk_dict.get(chunk_id, {})
            metadata.append({
                "index": i,
                "chunk_id": chunk_id,
                "content": chunk.get("content", ""),
                "metadata": chunk.get("metadata", {})
            })
        
        try:
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            print(f"âœ“ ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")
            print(f"  - í•­ëª© ìˆ˜: {len(metadata)}")
        except Exception as e:
            print(f"âœ— ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def load_metadata(self, metadata_path: str) -> Optional[List[Dict]]:
        """
        ë©”íƒ€ë°ì´í„° ë¡œë“œ
        
        Args:
            metadata_path: ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        
        Returns:
            ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” None
        """
        if not os.path.exists(metadata_path):
            print(f"âš  ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {metadata_path}")
            return None
        
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            print(f"âœ“ ë©”íƒ€ë°ì´í„° ë¡œë“œ: {metadata_path}")
            print(f"  - í•­ëª© ìˆ˜: {len(metadata)}")
            return metadata
        except Exception as e:
            print(f"âœ— ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def search(self, query: str, index: faiss.Index, metadata: List[Dict], 
               top_k: int = 10) -> List[Dict]:
        """
        ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            index: FAISS ì¸ë±ìŠ¤
            metadata: ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
        
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embed_text(query)
        query_embedding = query_embedding.reshape(1, -1).astype('float32')
        
        # FAISS ê²€ìƒ‰
        distances, indices = index.search(query_embedding, top_k)
        
        # ê²°ê³¼ êµ¬ì„±
        results = []
        for i, (idx, distance) in enumerate(zip(indices[0], distances[0])):
            if idx < len(metadata):
                result = {
                    "rank": i + 1,
                    "chunk_id": metadata[idx]["chunk_id"],
                    "content": metadata[idx]["content"],
                    "metadata": metadata[idx]["metadata"],
                    "distance": float(distance),
                    "similarity": float(1 / (1 + distance))  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                }
                results.append(result)
        
        return results
    
    def build_index_from_chunks(self, chunks_path: str, 
                                output_dir: str = None) -> Tuple[faiss.Index, List[Dict]]:

        """
        ì²­í¬ íŒŒì¼ì—ì„œ ì¸ë±ìŠ¤ êµ¬ì¶• (ì „ì²´ íŒŒì´í”„ë¼ì¸)
        
        Args:
            chunks_path: ì²­í¬ JSON íŒŒì¼ ê²½ë¡œ
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        
        Returns:
            (FAISS ì¸ë±ìŠ¤, ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸)
        """
        if output_dir is None:
            output_dir = f"data/vector_store/{self.institution}"
    
        print("\n" + "="*80)
        print("ğŸš€ FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘")
        print("="*80)
        
        # 1. ì²­í¬ ë¡œë“œ
        print("\n1ï¸âƒ£ ì²­í¬ ë¡œë“œ ì¤‘...")
        with open(chunks_path, 'r', encoding='utf-8') as f:
            chunks = json.load(f)
        print(f"âœ“ {len(chunks)}ê°œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
        
        # 2. ì„ë² ë”© ìƒì„±
        print("\n2ï¸âƒ£ ì„ë² ë”© ìƒì„± ì¤‘...")
        embeddings, chunk_ids = self.embed_chunks(chunks, batch_size=100)
        
        # 3. FAISS ì¸ë±ìŠ¤ ìƒì„±
        print("\n3ï¸âƒ£ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        index = self.create_faiss_index(embeddings)
        
        # 4. ì¸ë±ìŠ¤ ì €ì¥
        print("\n4ï¸âƒ£ ì¸ë±ìŠ¤ ì €ì¥ ì¤‘...")
        index_path = os.path.join(output_dir, "faiss_index.bin")
        self.save_index(index, index_path)
        
        # 5. ë©”íƒ€ë°ì´í„° ì €ì¥
        print("\n5ï¸âƒ£ ë©”íƒ€ë°ì´í„° ì €ì¥ ì¤‘...")
        metadata_path = os.path.join(output_dir, "metadata.json")
        self.save_metadata(chunks, chunk_ids, metadata_path)
        
        # 6. ë©”íƒ€ë°ì´í„° ë¡œë“œ (ê²€ì¦)
        metadata = self.load_metadata(metadata_path)
        
        print("\n" + "="*80)
        print("âœ… FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
        print("="*80)
        print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
        print(f"  - faiss_index.bin")
        print(f"  - metadata.json")
        print("="*80 + "\n")
        
        return index, metadata