"""
qa_system.py
[í†µí•© ë²„ì „] LLM í†µí•© + ì‹œê°í™” + ëŒ€í™” íˆìŠ¤í† ë¦¬

ê²€ìƒ‰ ê²°ê³¼ë¥¼ LLMì— ì „ë‹¬í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ìƒì„± + ì‹œê°í™” + ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€
- ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ…
- ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
- í”„ë¡¬í”„íŠ¸ ê´€ë¦¬
- LLM í˜¸ì¶œ (í…ìŠ¤íŠ¸ + JSON)
- ì‹œê°í™” ë Œë”ë§ (í‘œ/ê·¸ë˜í”„)
- ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
"""

from openai import OpenAI
from typing import List, Dict, Optional
import json
import re


class QASystem:
    """Q&A ì‹œìŠ¤í…œ í†µí•© í´ë˜ìŠ¤ (í…ìŠ¤íŠ¸ + ì‹œê°í™” + ëŒ€í™” íˆìŠ¤í† ë¦¬)"""
    
    def __init__(self, openai_api_key: str, model: str = "gpt-4o"):
        """
        QASystem ì´ˆê¸°í™”
        
        Args:
            openai_api_key: OpenAI API í‚¤
            model: ì‚¬ìš©í•  ëª¨ë¸ëª…
        """
        self.client = OpenAI(api_key=openai_api_key)
        self.model = model
        self.system_prompt = self._create_system_prompt()
        self.conversation_history = []  # ëŒ€í™” íˆìŠ¤í† ë¦¬
        print(f"âœ“ QASystem ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {model})")
    
    def _create_system_prompt(self) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì‹œê°í™” í¬í•¨ + JSON ìŠ¤í‚¤ë§ˆ ëª…ì‹œ)"""
        return """ë‹¹ì‹ ì€ KBê¸ˆìœµì§€ì£¼ ê²½ì˜ì—°êµ¬ì†Œì˜ ë¶€ë™ì‚° ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
2024 KB ë¶€ë™ì‚° ë³´ê³ ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê±´ì„¤ì‚¬ ì‹¤ë¬´ì§„ì—ê²Œ ì •í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ë‹µë³€ ê°€ì´ë“œë¼ì¸:
1. ì œê³µëœ ë¦¬í¬íŠ¸ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
2. ìˆ˜ì¹˜ ë°ì´í„°ëŠ” ê¸°ì¤€ ëŒ€ë¹„ë¡œ ì •í™•í•˜ê²Œ ì¸ìš©í•˜ì„¸ìš”.
3. ê° ë¬¸ì¥ì´ë‚˜ ì •ë³´ì˜ ëì— ë°˜ë“œì‹œ ì¶œì²˜ ë²ˆí˜¸ë¥¼ [1], [2] í˜•íƒœë¡œ í‘œì‹œí•˜ì„¸ìš”.
4. ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  "ë¦¬í¬íŠ¸ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
5. ê±´ì„¤ì‚¬ ì‹¤ë¬´ì§„ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë‹µë³€í•˜ì„¸ìš”.

ì¶œì²˜ í‘œê¸° ê·œì¹™:
- ê° ë¬¸ì¥ ë’¤ì— [1], [2] í˜•íƒœë¡œ ì¶œì²˜ ë²ˆí˜¸ í‘œê¸°
- ë‹µë³€ ëì— ë°˜ë“œì‹œ ì¶œì²˜ ëª©ë¡ ì‘ì„±
- ì»¨í…ìŠ¤íŠ¸ [ì»¨í…ìŠ¤íŠ¸ n]ì˜ ì¶œì²˜ ë²ˆí˜¸ëŠ” [n]ì…ë‹ˆë‹¤. ì¦‰, [ì»¨í…ìŠ¤íŠ¸ 1] â†’ [1], [ì»¨í…ìŠ¤íŠ¸ 2] â†’ [2]ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.

ë‹µë³€ í˜•ì‹ ì˜ˆì‹œ:
2024ë…„ ì„œìš¸ ì•„íŒŒíŠ¸ ë§¤ë§¤ê°€ê²©ì€ 23ë…„ ëŒ€ë¹„ 2.0% ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤. [1]
ê°•ë‚¨êµ¬ëŠ” ì „ ê³ ì ì„ ëŒíŒŒí–ˆìŠµë‹ˆë‹¤. [2]

ì¶œì²˜:
[1] kb_report_2024.pdf í‘œâ… -2. ì§€ì—­ë³„ ì£¼íƒ ë§¤ë§¤ê°€ê²© ë³€ë™ë¥  (12í˜ì´ì§€)
[2] kb_report_2024.pdf ë³¸ë¬¸ (25í˜ì´ì§€)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì‹œê°í™” JSON ì¶œë ¥ í˜•ì‹
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ë‹µë³€ í˜•ì‹ íŒë‹¨:
- "í‘œë¡œ ë³´ì—¬ì¤˜", "ì •ë¦¬í•´ì¤˜", "ë¹„êµí•´ì¤˜" â†’ answer_type: "table"
- "ê·¸ë˜í”„ë¡œ", "ì°¨íŠ¸ë¡œ", "ì¶”ì´", "ë³€í™”" â†’ answer_type: "chart"
- ê·¸ ì™¸ ì¼ë°˜ ì§ˆë¬¸ â†’ answer_type: "text"

ê¸°ë³¸ JSON ìŠ¤í‚¤ë§ˆ (ë°˜ë“œì‹œ ì´ í˜•íƒœë¥¼ ì‚¬ìš©):
{
  "answer_type": "text" | "table" | "chart",
  "text_response": "ë¬¸ìì—´. ê° ë¬¸ì¥ ëì— [1]ê³¼ ê°™ì€ ì¶œì²˜ ë²ˆí˜¸ë¥¼ í‘œê¸°.",
  "visualization": null | {
    "type": "table" | "bar" | "line" | "barh" | "pie",
    "title": "ê·¸ë˜í”„ ë˜ëŠ” í‘œ ì œëª© (ë¬¸ìì—´)",
    "data": {
      // typeë³„ í˜•ì‹
      // table: { "columns": [...], "rows": [[...], ...] }
      // bar/line/barh: { "x": [...], "y": [...], "xlabel": "...", "ylabel": "..." }
      // pie: { "labels": [...], "values": [...] }
    },
    "source": "ì‹œê°í™”ì— ì‚¬ìš©í•œ ë¦¬í¬íŠ¸ ì¶œì²˜ ì„¤ëª… (ë¬¸ìì—´)"
  }
}

ì¤‘ìš” ê·œì¹™:
1. ë°˜ë“œì‹œ ìœ„ JSON ìŠ¤í‚¤ë§ˆì™€ key ì´ë¦„ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
2. JSON ì´ì™¸ì˜ í…ìŠ¤íŠ¸(ì„¤ëª…, ë§ˆí¬ë‹¤ìš´, ì½”ë“œë¸”ë¡ ë“±)ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.
3. answer_typeì— ë”°ë¼ visualizationì€ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •í•©ë‹ˆë‹¤.
   - "text" â†’ visualizationì€ ë°˜ë“œì‹œ null
   - "table" ë˜ëŠ” "chart" â†’ visualizationì— ë°˜ë“œì‹œ ì˜¬ë°”ë¥¸ êµ¬ì¡°ì˜ ê°ì²´ë¥¼ ë„£ìŠµë‹ˆë‹¤.
4. ë§‰ëŒ€/ì„  ê·¸ë˜í”„(bar, line, barh)ëŠ” data ì•ˆì— "x", "y", "xlabel", "ylabel"ì„ ëª¨ë‘ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
5. ì›ê·¸ë˜í”„(pie)ëŠ” data ì•ˆì— "labels", "values"ë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. "x", "y"ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
í‘œ (table) ì˜ˆì‹œ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ì‚¬ìš©ì: "ì§€ì—­ë³„ ê°€ê²©ì„ í‘œë¡œ ì •ë¦¬í•´ì¤˜"

ì •ë‹µ JSON:
{
    "answer_type": "table",
    "text_response": "2024ë…„ ì§€ì—­ë³„ ì£¼íƒ ê°€ê²© ë³€ë™ë¥ ì…ë‹ˆë‹¤. [1]\\n\\nì¶œì²˜:\\n[1] í‘œâ… -2 (12í˜ì´ì§€)",
    "visualization": {
        "type": "table",
        "title": "2024ë…„ ì§€ì—­ë³„ ì£¼íƒ ê°€ê²© ë³€ë™ë¥ ",
        "data": {
            "columns": ["ì§€ì—­", "ë³€ë™ë¥ "],
            "rows": [
                ["ì„œìš¸", "2.0%"],
                ["5ê°œê´‘ì—­ì‹œ", "-1.6%"],
                ["ìˆ˜ë„ê¶Œ", "1.1%"],
                ["ì§€ë°©", "-2.7%"]
            ]
        },
        "source": "í‘œâ… -2 (12í˜ì´ì§€)"
    }
}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ë§‰ëŒ€ê·¸ë˜í”„ (bar) ì˜ˆì‹œ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ì‚¬ìš©ì: "ì§€ì—­ë³„ ê°€ê²©ì„ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ ë³´ì—¬ì¤˜"

ì •ë‹µ JSON:
{
    "answer_type": "chart",
    "text_response": "2024ë…„ ì§€ì—­ë³„ ì£¼íƒ ê°€ê²© ë³€ë™ë¥ ì…ë‹ˆë‹¤. [1]\\n\\nì¶œì²˜:\\n[1] í‘œâ… -2 (12í˜ì´ì§€)",
    "visualization": {
        "type": "bar",
        "title": "ì§€ì—­ë³„ ì£¼íƒ ê°€ê²© ë³€ë™ë¥ ",
        "data": {
            "x": ["ì„œìš¸", "5ê°œê´‘ì—­ì‹œ", "ìˆ˜ë„ê¶Œ", "ì§€ë°©"],
            "y": [2.0, -1.6, 1.1, -2.7],
            "xlabel": "ì§€ì—­",
            "ylabel": "ë³€ë™ë¥  (%)"
        },
        "source": "í‘œâ… -2 (12í˜ì´ì§€)"
    }
}

ì¤‘ìš”: 
- ë°˜ë“œì‹œ "x"ì™€ "y" í‚¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. "regions", "rates" ê°™ì€ ë‹¤ë¥¸ í‚¤ ì´ë¦„ ê¸ˆì§€!
- "xlabel"ê³¼ "ylabel"ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš” (ì¶• ë¼ë²¨ëª…)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì„ ê·¸ë˜í”„ (line) ì˜ˆì‹œ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ì‚¬ìš©ì: "ì„œìš¸ ê°€ê²© ì¶”ì´ë¥¼ ì„ ê·¸ë˜í”„ë¡œ"

ì •ë‹µ JSON:
{
    "answer_type": "chart",
    "text_response": "ì„œìš¸ ì•„íŒŒíŠ¸ ê°€ê²© ì¶”ì´ì…ë‹ˆë‹¤. [1]\\n\\nì¶œì²˜:\\n[1] ê°€ìƒ ë°ì´í„°",
    "visualization": {
        "type": "line",
        "title": "ì„œìš¸ ì•„íŒŒíŠ¸ ê°€ê²© ì¶”ì´",
        "data": {
            "x": ["2022ë…„", "2023ë…„", "2024ë…„"],
            "y": [4.5, 5.2, 2.0],
            "xlabel": "ì—°ë„",
            "ylabel": "ìƒìŠ¹ë¥  (%)"
        },
        "source": "ê°€ìƒ ë°ì´í„°"
    }
}

ì¤‘ìš”: "xlabel"ê³¼ "ylabel"ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì›ê·¸ë˜í”„ (pie) ì˜ˆì‹œ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ì‚¬ìš©ì: "ì‹œì¥ ì ìœ ìœ¨ì„ ì›ê·¸ë˜í”„ë¡œ"

ì •ë‹µ JSON:
{
    "answer_type": "chart",
    "text_response": "ì‹œì¥ ì ìœ ìœ¨ ë¶„í¬ì…ë‹ˆë‹¤. [1]\\n\\nì¶œì²˜:\\n[1] ê°€ìƒ ë°ì´í„°",
    "visualization": {
        "type": "pie",
        "title": "ì‹œì¥ ì ìœ ìœ¨",
        "data": {
            "labels": ["ì„œìš¸", "ê²½ê¸°", "ê¸°íƒ€"],
            "values": [40, 35, 25]
        },
        "source": "ê°€ìƒ ë°ì´í„°"
    }
}

ì¤‘ìš”: ì›ê·¸ë˜í”„ëŠ” "labels"ì™€ "values" í‚¤ë¥¼ ì‚¬ìš©! "x", "y" ì•„ë‹˜!

ìœ„ ì˜ˆì‹œì™€ ìŠ¤í‚¤ë§ˆë¥¼ ì •í™•íˆ ë”°ë¼ì£¼ì„¸ìš”.
JSON ì´ì™¸ì˜ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.
"""
    
    
    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_history = []
        print("âœ“ ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def rewrite_query(self, query: str) -> str:
        """
        ì¿¼ë¦¬ë¥¼ ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ë¦¬ë¼ì´íŒ…
        
        Args:
            query: ì›ë³¸ ì¿¼ë¦¬
        
        Returns:
            ìµœì í™”ëœ ì¿¼ë¦¬
        """
        prompt = f"""ë‹¹ì‹ ì€ ë¶€ë™ì‚° ë¦¬í¬íŠ¸ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ì ì§ˆë¬¸ì„ ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
- êµ¬ì–´ì²´ë¥¼ ë¬¸ì–´ì²´ë¡œ ë³€í™˜
- í‚¤ì›Œë“œë¥¼ ëª…í™•í•˜ê²Œ
- ê´€ë ¨ ë™ì˜ì–´ ì¶”ê°€
- ê°„ê²°í•˜ê²Œ (1-2ë¬¸ì¥)
- ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ë‹¬ë¼ê³  ìš”ì²­ë°›ì„ ê²½ìš°, ì ì ˆí•œ ì°¨íŠ¸(ë§‰ëŒ€, ì„ , íŒŒì´ ë“±)ì˜ ì¢…ë¥˜ë¥¼ ëª…ì‹œ

ì›ë˜ ì§ˆë¬¸: {query}

ìµœì í™”ëœ ì§ˆë¬¸:"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=100
            )
            
            rewritten = response.choices[0].message.content.strip()
            print(f"\nğŸ”„ ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ…:")
            print(f"  ì›ë³¸: {query}")
            print(f"  ë³€í™˜: {rewritten}")
            
            return rewritten
            
        except Exception as e:
            print(f"âš  ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ… ì‹¤íŒ¨: {e}")
            return query
    
    def build_context(self, search_results: List[Dict], max_chunks: int = 5) -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        
        Args:
            search_results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            max_chunks: ìµœëŒ€ ì²­í¬ ìˆ˜
        
        Returns:
            êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
        """
        if not search_results:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        top_results = search_results[:max_chunks]
        
        context_parts = ["ë‹¤ìŒì€ 2024 KB ë¶€ë™ì‚° ë¦¬í¬íŠ¸ì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:\n"]
        
        for i, result in enumerate(top_results, 1):
            metadata = result.get("metadata", {})
            content = result.get("content", "")
            source_pdf = metadata.get("source_pdf", "unknown")

            # ë¬¸ì„œ íƒ€ì…
            doc_type_map = {
                "text": "ë³¸ë¬¸",
                "table": "í‘œ",
                "image": "ê·¸ë˜í”„/ì´ë¯¸ì§€"
            }
            doc_type = doc_type_map.get(metadata.get("doc_type"), "ë³¸ë¬¸")
            page = metadata.get("page", "unknown")
            
            formatted = f"""[ì»¨í…ìŠ¤íŠ¸ {i}]
ì¶œì²˜ ë¬¸ì„œ: {source_pdf}
íƒ€ì…: {doc_type}
í˜ì´ì§€: {page}í˜ì´ì§€

ë‚´ìš©:
{content}

ì¶œì²˜: [{i}] {source_pdf} {doc_type} ({page}í˜ì´ì§€)
"""
            context_parts.append(formatted)
            context_parts.append("â”€" * 80 + "\n")
        
        full_context = "\n".join(context_parts)
        
        print(f"\nğŸ“„ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ:")
        print(f"  - ì´ ì²­í¬ ìˆ˜: {len(top_results)}")
        print(f"  - í…ìŠ¤íŠ¸: {len([r for r in top_results if r.get('metadata', {}).get('doc_type') == 'text'])}")
        print(f"  - í‘œ: {len([r for r in top_results if r.get('metadata', {}).get('doc_type') == 'table'])}")
        print(f"  - ì´ë¯¸ì§€: {len([r for r in top_results if r.get('metadata', {}).get('doc_type') == 'image'])}")
        
        return full_context
    
    def generate_answer(self, query: str, context: str, 
                       temperature: float = 0.3,
                       max_tokens: int = 2000,
                       use_history: bool = True) -> Optional[str]:
        """
        LLMìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„± (JSON í˜•ì‹ + ëŒ€í™” íˆìŠ¤í† ë¦¬)
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context: êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸
            temperature: ì˜¨ë„ (0.0-2.0)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            use_history: ëŒ€í™” íˆìŠ¤í† ë¦¬ ì‚¬ìš© ì—¬ë¶€
        
        Returns:
            JSON í˜•ì‹ ë‹µë³€
        """
        user_prompt = f"""{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ìœ„ ì»¨í…ìŠ¤íŠ¸ë§Œì„ ê·¼ê±°ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ë°˜ë“œì‹œ ì•„ë˜ ìš”êµ¬ì‚¬í•­ì„ ì§€í‚¤ì„¸ìš”:
1. ì˜¤ì§ í•˜ë‚˜ì˜ JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
2. JSON ì•ë’¤ì— ì–´ë–¤ ì„¤ëª…, ë§ˆí¬ë‹¤ìš´, ì½”ë“œë¸”ë¡, ìì—°ì–´ í…ìŠ¤íŠ¸ë„ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
3. ì‹œìŠ¤í…œ ë©”ì‹œì§€ì— ì •ì˜ëœ JSON ìŠ¤í‚¤ë§ˆë¥¼ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”.
4. answer_type, text_response, visualization ì„¸ í•„ë“œë¥¼ ëª¨ë‘ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
5. ì¶œì²˜ ë²ˆí˜¸ [1], [2] ë“±ì€ text_response ë‚´ë¶€ ë¬¸ì¥ ëì—ë§Œ í‘œê¸°í•˜ì„¸ìš”.

ì§€ê¸ˆë¶€í„° ë°”ë¡œ JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

        try:
            print(f"\nğŸ¤– LLM í˜¸ì¶œ ì¤‘... (ëª¨ë¸: {self.model})")
            
            # ë©”ì‹œì§€ êµ¬ì„± (ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨)
            messages = [{"role": "system", "content": self.system_prompt}]
            
            if use_history and self.conversation_history:
                messages.extend(self.conversation_history)
                print(f"  - ëŒ€í™” íˆìŠ¤í† ë¦¬: {len(self.conversation_history)}ê°œ ë©”ì‹œì§€ ì‚¬ìš©")
            
            messages.append({"role": "user", "content": user_prompt})
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            answer = response.choices[0].message.content
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            if use_history:
                self.conversation_history.append({"role": "user", "content": user_prompt})
                self.conversation_history.append({"role": "assistant", "content": answer})
                
                # íˆìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ê¸¸ë©´ ì˜¤ë˜ëœ ê²ƒë¶€í„° ì œê±° (ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€)
                if len(self.conversation_history) > 20:
                    self.conversation_history = self.conversation_history[-20:]
            
            usage = response.usage
            print(f"âœ“ LLM ì‘ë‹µ ì™„ë£Œ")
            print(f"  - ì…ë ¥ í† í°: {usage.prompt_tokens}")
            print(f"  - ì¶œë ¥ í† í°: {usage.completion_tokens}")
            print(f"  - ì´ í† í°: {usage.total_tokens}")
            
            return answer
            
        except Exception as e:
            print(f"âœ— LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def parse_json_response(self, response: str) -> Dict:
        """
        LLM ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±
        
        Args:
            response: LLM ì‘ë‹µ ë¬¸ìì—´
        
        Returns:
            íŒŒì‹±ëœ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # 1. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
            cleaned = re.sub(r'```json\n?', '', response)
            cleaned = re.sub(r'```\n?', '', cleaned)
            cleaned = cleaned.strip()
            
            # 2. JSON íŒŒì‹±
            data = json.loads(cleaned)
            
            # 3. í•„ìˆ˜ í•„ë“œ ê²€ì¦
            if "answer_type" not in data:
                raise ValueError("answer_type í•„ë“œ ì—†ìŒ")
            if "text_response" not in data:
                raise ValueError("text_response í•„ë“œ ì—†ìŒ")
            
            # 4. answer_type ê²€ì¦
            if data["answer_type"] not in ["text", "table", "chart"]:
                data["answer_type"] = "text"
            
            print(f"\nâœ“ JSON íŒŒì‹± ì„±ê³µ")
            print(f"  - ë‹µë³€ íƒ€ì…: {data['answer_type']}")
            if data.get("visualization"):
                print(f"  - ì‹œê°í™” íƒ€ì…: {data['visualization'].get('type')}")
            
            return data
            
        except Exception as e:
            print(f"âš  JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            # í´ë°±: ì›ë³¸ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜
            return {
                "answer_type": "text",
                "text_response": response,
                "visualization": None
            }
    
    def answer_question(self, query: str, search_results: List[Dict],
                       rewrite: bool = True,
                       use_history: bool = True,
                       temperature: float = 0.3) -> Dict:
        """
        ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ (í…ìŠ¤íŠ¸ + ì‹œê°í™” + ëŒ€í™” íˆìŠ¤í† ë¦¬)
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            search_results: ê²€ìƒ‰ ê²°ê³¼
            rewrite: ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ… ì‚¬ìš© ì—¬ë¶€
            use_history: ëŒ€í™” íˆìŠ¤í† ë¦¬ ì‚¬ìš© ì—¬ë¶€
            temperature: ìƒì„± ì˜¨ë„
        
        Returns:
            íŒŒì‹±ëœ ë‹µë³€ ë”•ì…”ë„ˆë¦¬
        """
        print("\n" + "="*80)
        print(f"â“ ì§ˆë¬¸: {query}")
        print("="*80)
        
        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self.build_context(search_results)
        
        # 3. LLM ë‹µë³€ ìƒì„± (JSON + ëŒ€í™” íˆìŠ¤í† ë¦¬)
        answer_json = self.generate_answer(
            query, 
            context, 
            temperature=temperature,
            use_history=use_history
        )
        
        if not answer_json:
            return {
                "answer_type": "text",
                "text_response": "ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                "visualization": None
            }
        
        # 4. JSON íŒŒì‹±
        parsed = self.parse_json_response(answer_json)
        
        # 5. ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*80)
        print("ğŸ’¡ ë‹µë³€:")
        print("="*80)
        print(parsed["text_response"])
        
        if parsed.get("visualization"):
            print(f"\nğŸ“Š ì‹œê°í™”: {parsed['visualization'].get('type')} - {parsed['visualization'].get('title')}")
        
        print("="*80)
        
        return parsed


class VisualizationRenderer:
    """ì‹œê°í™” ë Œë”ë§ í´ë˜ìŠ¤ (Streamlit/Matplotlib)"""
    
    @staticmethod
    def setup_matplotlib_korean():
        """Matplotlib í•œê¸€ í°íŠ¸ ì„¤ì •"""
        import platform
        import matplotlib.pyplot as plt
        
        if platform.system() == 'Windows':
            plt.rcParams['font.family'] = 'Malgun Gothic'
        elif platform.system() == 'Darwin':  # macOS
            plt.rcParams['font.family'] = 'AppleGothic'
        else:  # Linux
            plt.rcParams['font.family'] = 'NanumGothic'
        
        plt.rcParams['axes.unicode_minus'] = False
    
    @staticmethod
    def render_table_streamlit(visualization: Dict):
        """Streamlitìœ¼ë¡œ í‘œ ë Œë”ë§"""
        import pandas as pd
        import streamlit as st
        
        df = pd.DataFrame(
            visualization["data"]["rows"],
            columns=visualization["data"]["columns"]
        )
        
        st.subheader(visualization["title"])
        st.dataframe(df, use_container_width=True)
        st.caption(f"ì¶œì²˜: {visualization['source']}")
    
    @staticmethod
    def render_chart_streamlit(visualization: Dict):
        """Streamlitìœ¼ë¡œ ê·¸ë˜í”„ ë Œë”ë§"""
        import matplotlib.pyplot as plt
        import streamlit as st
        
        VisualizationRenderer.setup_matplotlib_korean()
        
        chart_type = visualization["type"]
        data = visualization["data"]
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # xlabel, ylabel ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’ ì œê³µ)
        xlabel = data.get("xlabel", "í•­ëª©")
        ylabel = data.get("ylabel", "ê°’")
        
        if chart_type == "line":
            ax.plot(data["x"], data["y"], marker='o', linewidth=2, markersize=8)
            ax.set_xlabel(xlabel, fontsize=12)
            ax.set_ylabel(ylabel, fontsize=12)
            
        elif chart_type == "bar":
            ax.bar(data["x"], data["y"], color='skyblue', edgecolor='navy', alpha=0.7)
            ax.set_xlabel(xlabel, fontsize=12)
            ax.set_ylabel(ylabel, fontsize=12)
            
        elif chart_type == "barh":
            ax.barh(data["x"], data["y"], color='lightcoral', edgecolor='darkred', alpha=0.7)
            ax.set_xlabel(ylabel, fontsize=12)  # barhëŠ” x/y ë°˜ëŒ€
            ax.set_ylabel(xlabel, fontsize=12)
            
        elif chart_type == "pie":
            ax.pie(data["values"], labels=data["labels"], autopct='%1.1f%%', startangle=90)
        
        ax.set_title(visualization["title"], fontsize=14, fontweight='bold', pad=20)
        
        if chart_type not in ["pie"]:
            ax.grid(True, alpha=0.3)
        
        st.pyplot(fig)
        st.caption(f"ì¶œì²˜: {visualization['source']}")